{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b978e61c",
   "metadata": {},
   "source": [
    "# Implementazione del task 1 - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c86dec",
   "metadata": {},
   "source": [
    "In uno scenario reale, le attività preliminari fondamentali che precedono qualsiasi operazione di data mining consistono in:\n",
    "1. Identificare le informazioni rilevanti per il task di data mining che si vorrà implementare.\n",
    "2. Estrarre le informazioni identificate dalla fonte di dati (ad esempio, il dataset *fitbit*).\n",
    "3. Integrare le informazioni estratte in un *Data Store* consistente.\n",
    "\n",
    "Dallo stesso Data Store si potranno poi eseguire vari task di data mining (ad esempio, estrazione di regole e process mining).\n",
    "\n",
    "Il task di *Data Preprocessing* si occupa quindi di selezionare, trasformare e caricare i dati in un Data Store (o Prepared Data) che possa gestire formati di dati specifici per vari task di data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c5060",
   "metadata": {},
   "source": [
    "## Struttura del dataset fitbit\n",
    "Il sito [https://datasets.simula.no/pmdata/](https://datasets.simula.no/pmdata/) presenta tutti i riferimenti sulla struttura del dataset fitbit.\n",
    "\n",
    "Ognuno dei 16 partecipanti presenti nel dataset ha una cartella `pNUMBER` associata che contiene varie informazioni relative ad un periodo di sei mesi durante il quale ha accettato di auto-monitorarsi. Ai fini della nostra analisi verranno utilizzati i file contenuti nelle cartelle `pNUMBER/fitbit` e `pNUMBER/pmsys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec6c276-e1e1-4425-bd4c-b5c15238e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import pandas as pd\n",
    "import json as json\n",
    "import datetime as dt\n",
    "import dateutil as du\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aef34d2-12aa-421c-95ee-d5ff602054a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABILI\n",
    "PATH = './pmdata/'\n",
    "people = range(1,17)  #[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ca414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI DI SUPPORTO\n",
    "# Questa funzione serve a normalizzare il dataset exercise sostituendo i NaN con il valore medio della colonna\n",
    "def fill_NaN_for_excercise_0(ex_0):\n",
    "    ex_0 = ex_0.copy()\n",
    "    # dall'analisi fatta sui dati sappiamo già quali sono le colonne\n",
    "    column = ['elavation_gain', 'steps', 'average_heart_rate']\n",
    "    means = []\n",
    "    for col in column:\n",
    "        if col != 'elavation_gain':\n",
    "            means.append(int(ex_0[col].mean()))\n",
    "        else:\n",
    "            means.append(ex_0[col].mean())\n",
    "    \n",
    "    for i, col in enumerate(column):\n",
    "        ex_0[col].fillna(value=means[i], inplace=True)\n",
    "    return ex_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3332dced-bdef-4baf-845c-dab4a6fde629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI (per i file json)\n",
    "def calories_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        calories_file = 'calories.json'\n",
    "        \n",
    "        # Leggiamo il file json per trasformarlo in un dataframe\n",
    "        with open(p_folder + calories_file) as file:\n",
    "            dict_cal = json.load(file)\n",
    "        \n",
    "        for d in dict_cal:\n",
    "            # Convertiamo i dati nei loro tipi appropriati e assegniamo loro dei nomi significativi\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['calories'] = float(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "            \n",
    "        # Trasformiamo il dizionario appena costruito in un dataframe\n",
    "        df_cal = pd.DataFrame.from_dict(dict_cal)\n",
    "        df_cal['partecipant'] = p_id\n",
    "        df_cal = df_cal.set_index(['partecipant','TS'])\n",
    "        dfs.append(df_cal)\n",
    "    r = pd.concat(dfs)  # unisce i diversi dataframe in uno unico\n",
    "    r = r.sort_index()  # ordina il dataframe rispetto all'indice formato da partecipante e TS\n",
    "    return r\n",
    "\n",
    "def sedentary_minutes_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'sedentary_minutes.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['sedentary_minutes'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def distance_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'distance.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['distance'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def heart_rate_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'heart_rate.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['bpm'] = int(d['value']['bpm'])\n",
    "            d['confidence'] = int(d['value']['confidence'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def steps_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'steps.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['steps'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def lightly_active_minutes_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'lightly_active_minutes.json'\n",
    "        \n",
    "        # provo ad aprire il file, se non c'è salto quel partecipante\n",
    "        try:\n",
    "            _ = open(p_folder + file)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['lightly_active_minutes'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def time_in_heart_rate_zones_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'time_in_heart_rate_zones.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['BELOW_DEFAULT_ZONE_1'] = float(d['value']['valuesInZones']['BELOW_DEFAULT_ZONE_1'])\n",
    "            d['IN_DEFAULT_ZONE_1'] = float(d['value']['valuesInZones']['IN_DEFAULT_ZONE_1'])\n",
    "            d['IN_DEFAULT_ZONE_2'] = float(d['value']['valuesInZones']['IN_DEFAULT_ZONE_2'])\n",
    "            d['IN_DEFAULT_ZONE_3'] = float(d['value']['valuesInZones']['IN_DEFAULT_ZONE_3'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def moderately_active_minutes_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'moderately_active_minutes.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['moderately_active_minutes'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def very_active_minutes_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'very_active_minutes.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['very_active_minutes'] = int(d['value'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def resting_heart_rate_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'resting_heart_rate.json'\n",
    "        \n",
    "        # provo ad aprire il file, se non c'è salto quel partecipante\n",
    "        try:\n",
    "            _ = open(p_folder + file)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        for d in dictionary:\n",
    "            d['TS'] = du.parser.parse(d['dateTime'])\n",
    "            d['resting_heart_rate'] = float(d['value']['value'])\n",
    "            d['resting_heart_rate_error'] = float(d['value']['error'])\n",
    "            d.pop('dateTime')\n",
    "            d.pop('value')\n",
    "        df = pd.DataFrame.from_dict(dictionary)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def exercise_to_df(root_path, partecipants):\n",
    "    # abbiamo due classi e ritorniamo due DataFrame\n",
    "    dfs = [[], []]\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'exercise.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        \n",
    "        # dividiamo il dataset in due gruppi\n",
    "        final_dict_list = [[], []]\n",
    "        for d in dictionary:\n",
    "            final_dict = {}\n",
    "            final_dict['TS'] = du.parser.parse(d['startTime'])\n",
    "            final_dict['activity_name'] = d['activityName']\n",
    "            final_dict['activity_level_sedentary_minutes'] = int(d['activityLevel'][0]['minutes'])\n",
    "            final_dict['activity_level_lightly_minutes'] = int(d['activityLevel'][1]['minutes'])\n",
    "            final_dict['activity_level_fairly_minutes'] = int(d['activityLevel'][2]['minutes'])\n",
    "            final_dict['activity_level_very_minutes'] = int(d['activityLevel'][3]['minutes'])\n",
    "            final_dict['calories'] = int(d['calories'])\n",
    "            final_dict['active_duration'] = int(d['activeDuration'])\n",
    "            \n",
    "            # questo tipo di implementazione deriva dagli studi fatti sul dataset che hanno rilevato questo:\n",
    "            # la maggior parte (più del 99%) delle istanze che non hanno i valori come\n",
    "            # 'activity_level_sedentary_minutes' non hanno neanche 'elevationGain', 'steps' e 'averageHeartRate'.\n",
    "            # Per cui noi li inseriamo solo per le instanze che hanno i valori come 'activity_level_sedentary_minutes'.\n",
    "            try:\n",
    "                _class = 0\n",
    "                \n",
    "                #\"heartRateZones\": [{\"name\": \"Out of Range\", \"min\": 30, \"max\": 86, \"minutes\": 1},\n",
    "                #{\"name\": \"Fat Burn\", \"min\": 86, \"max\": 121, \"minutes\": 20},\n",
    "                #{\"name\": \"Cardio\", \"min\": 121, \"max\": 147, \"minutes\": 0},\n",
    "                #{\"name\": \"Peak\", \"min\": 147, \"max\": 220, \"minutes\": 0}]\n",
    "                final_dict['hrz_out_of_range_minutes'] = int(d['heartRateZones'][0]['minutes'])\n",
    "                final_dict['hrz_out_of_range_min'] = int(d['heartRateZones'][0]['min'])\n",
    "                final_dict['hrz_out_of_range_max'] = int(d['heartRateZones'][0]['max'])\n",
    "                \n",
    "                final_dict['hrz_fat_burn_minutes'] = int(d['heartRateZones'][1]['minutes'])\n",
    "                final_dict['hrz_fat_burn_min'] = int(d['heartRateZones'][1]['min'])\n",
    "                final_dict['hrz_fat_burn_max'] = int(d['heartRateZones'][1]['max'])\n",
    "                \n",
    "                final_dict['hrz_cardio_minutes'] = int(d['heartRateZones'][2]['minutes'])\n",
    "                final_dict['hrz_cardio_min'] = int(d['heartRateZones'][2]['min'])\n",
    "                final_dict['hrz_cardio_max'] = int(d['heartRateZones'][2]['max'])\n",
    "                \n",
    "                final_dict['hrz_peak_minutes'] = int(d['heartRateZones'][3]['minutes'])\n",
    "                final_dict['hrz_peak_min'] = int(d['heartRateZones'][3]['min'])\n",
    "                final_dict['hrz_peak_max'] = int(d['heartRateZones'][3]['max'])\n",
    "\n",
    "                # Nota: non tutte le attività hanno elevationGain, steps ecc. Per cui proviamo ad inserirlo e se non\n",
    "                #       si riesce lo impostiamo a None.\n",
    "                try:\n",
    "                    final_dict['elavation_gain'] = float(d['elevationGain'])\n",
    "                except:\n",
    "                    final_dict['elavation_gain'] = None\n",
    "\n",
    "                try:\n",
    "                    final_dict['steps'] = int(d['steps'])\n",
    "                except:\n",
    "                    final_dict['steps'] = None\n",
    "\n",
    "                try:\n",
    "                    final_dict['average_heart_rate'] = int(d['averageHeartRate'])\n",
    "                except:\n",
    "                    final_dict['average_heart_rate'] = None\n",
    "\n",
    "            except:\n",
    "                _class = 1\n",
    "\n",
    "            final_dict_list[_class].append(final_dict)\n",
    "        \n",
    "        # per le due classi\n",
    "        df_class_0 = pd.DataFrame.from_dict(final_dict_list[0])\n",
    "        df_class_1 = pd.DataFrame.from_dict(final_dict_list[1])\n",
    "        \n",
    "        # per eseguire l'aggiunta della colonna e l'impostazione dell'indice controllo prima se il dataset ha delle entry\n",
    "        # altrimenti darà errore.\n",
    "\n",
    "        if len(df_class_0) > 0:\n",
    "            df_class_0['partecipant'] = p_id\n",
    "            df_class_0 = df_class_0.set_index(['partecipant','TS'])\n",
    "            dfs[0].append(df_class_0)\n",
    "        if len(df_class_1) > 0:\n",
    "            df_class_1['partecipant'] = p_id\n",
    "            df_class_1 = df_class_1.set_index(['partecipant','TS'])\n",
    "            dfs[1].append(df_class_1)\n",
    "    \n",
    "    # normalizzo exercise_0. La funzione agisce direttamente sul dataset e quindi la funzione non restituisce niente\n",
    "    return fill_NaN_for_excercise_0(pd.concat(dfs[0]).sort_index()), pd.concat(dfs[1]).sort_index()\n",
    "    \n",
    "\n",
    "def sleep_to_df(root_path, partecipants):\n",
    "    # abbiamo due classi e ritorniamo due DataFrame\n",
    "    dfs = [[],[]]\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/fitbit/'\n",
    "        file = 'sleep.json'\n",
    "        with open(p_folder + file) as file:\n",
    "            dictionary = json.load(file)\n",
    "        \n",
    "        # dividiamo il dataset in due gruppi:\n",
    "        # - il primo dataset comprende solo coloro che hanno dormito\n",
    "        # - il secondo dataset comprende coloro che si sono svegliati durante la notte\n",
    "        final_dict_list = [[], []]\n",
    "        for d in dictionary:\n",
    "            final_dict = {}\n",
    "            final_dict['TS'] = du.parser.parse(d['startTime'])\n",
    "            final_dict['duration'] = d['duration']\n",
    "            final_dict['minutes_to_fall_asleep'] = int(d['minutesToFallAsleep'])\n",
    "            final_dict['minutes_asleep'] = int(d['minutesAsleep'])\n",
    "            final_dict['minutes_awake'] = int(d['minutesAwake'])\n",
    "            final_dict['minutes_after_wakeup'] = int(d['minutesAfterWakeup'])\n",
    "            final_dict['efficiency'] = int(d['efficiency'])\n",
    "            final_dict['main_sleep'] = bool(d['mainSleep'])\n",
    "\n",
    "            try:\n",
    "                _class = 0\n",
    "                final_dict['level_deep_count'] = int(d['levels']['summary']['deep']['count'])\n",
    "                final_dict['level_deep_minutes'] = int(d['levels']['summary']['deep']['minutes'])\n",
    "                \n",
    "                final_dict['level_wake_count'] = int(d['levels']['summary']['wake']['count'])\n",
    "                final_dict['level_wake_minutes'] = int(d['levels']['summary']['wake']['minutes'])\n",
    "                \n",
    "                final_dict['level_light_count'] = int(d['levels']['summary']['light']['count'])\n",
    "                final_dict['level_light_minutes'] = int(d['levels']['summary']['light']['minutes'])\n",
    "                \n",
    "                final_dict['level_rem_counts'] = int(d['levels']['summary']['rem']['count'])\n",
    "                final_dict['level_rem_minutes'] = int(d['levels']['summary']['rem']['minutes'])\n",
    "            except:\n",
    "                _class = 1\n",
    "                final_dict['level_restless_count'] = int(d['levels']['summary']['restless']['count'])\n",
    "                final_dict['level_restless_minutes'] = int(d['levels']['summary']['restless']['minutes'])\n",
    "                \n",
    "                final_dict['level_awake_count'] = int(d['levels']['summary']['awake']['count'])\n",
    "                final_dict['level_awake_minutes'] = int(d['levels']['summary']['awake']['minutes'])\n",
    "                \n",
    "                final_dict['level_asleep_count'] = int(d['levels']['summary']['asleep']['count'])\n",
    "                final_dict['level_asleep_minutes'] = int(d['levels']['summary']['asleep']['minutes'])\n",
    "\n",
    "            final_dict_list[_class].append(final_dict)\n",
    "        \n",
    "        # per le due classi\n",
    "        df_class_0 = pd.DataFrame.from_dict(final_dict_list[0])\n",
    "        df_class_1 = pd.DataFrame.from_dict(final_dict_list[1])\n",
    "        \n",
    "        # per eseguire l'aggiunta della colonna e l'impostazione dell'indice controllo prima se il dataset ha delle entry\n",
    "        # altrimenti darà errore.\n",
    "\n",
    "        if len(df_class_0) > 0:\n",
    "            df_class_0['partecipant'] = p_id\n",
    "            df_class_0 = df_class_0.set_index(['partecipant','TS'])\n",
    "            dfs[0].append(df_class_0)\n",
    "        if len(df_class_1) > 0:\n",
    "            df_class_1['partecipant'] = p_id\n",
    "            df_class_1 = df_class_1.set_index(['partecipant','TS'])\n",
    "            dfs[1].append(df_class_1)\n",
    "    \n",
    "    return pd.concat(dfs[0]).sort_index(), pd.concat(dfs[1]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbbb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI (per i file csv)\n",
    "def srpe_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/pmsys/'\n",
    "        file = 'srpe.csv'\n",
    "        with open(p_folder + file) as file:\n",
    "            lines = csv.reader(file, delimiter=',')\n",
    "        \n",
    "            first_line = True\n",
    "            final_dict_list = []\n",
    "            for l in lines:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    continue\n",
    "                else:\n",
    "                    final_dict = {}\n",
    "                    final_dict['TS'] = du.parser.parse(l[0])\n",
    "                    final_dict['activity_names'] = list(eval(l[1]))\n",
    "                    final_dict['perceived_exertion'] = int(l[2]) if l[2] != '' else None\n",
    "                    final_dict['duration_min'] = int(l[3]) if l[3] != '' else None\n",
    "                    \n",
    "                    # le righe che hanno colonne vuote non verranno inserite nel df (vedi p16)\n",
    "                    if len(final_dict['activity_names']) > 0:\n",
    "                        final_dict_list.append(final_dict)\n",
    "\n",
    "        if len(final_dict_list) > 0:\n",
    "            df = pd.DataFrame.from_dict(final_dict_list)\n",
    "            df['partecipant'] = p_id\n",
    "            df = df.set_index(['partecipant','TS'])\n",
    "            dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def wellness_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/pmsys/'\n",
    "        file = 'wellness.csv'\n",
    "        with open(p_folder + file) as file:\n",
    "            lines = csv.reader(file, delimiter=',')\n",
    "        \n",
    "            first_line = True\n",
    "            final_dict_list = []\n",
    "            for l in lines:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    continue\n",
    "                else:\n",
    "                    final_dict = {}\n",
    "                    final_dict['TS'] = du.parser.parse(l[0])\n",
    "                    final_dict['fatigue'] = int(l[1])\n",
    "                    final_dict['mood'] = int(l[2])\n",
    "                    final_dict['readiness'] = int(l[3])\n",
    "                    final_dict['sleep_duration_h'] = int(l[4])\n",
    "                    final_dict['sleep_quality'] = int(l[5])\n",
    "                    final_dict['soreness'] = int(l[6])\n",
    "                    final_dict['soreness_area'] = list(eval(l[7]))\n",
    "                    final_dict['stress'] = int(l[8])\n",
    "                    final_dict_list.append(final_dict)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(final_dict_list)\n",
    "        df['partecipant'] = p_id\n",
    "        df = df.set_index(['partecipant','TS'])\n",
    "        dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r\n",
    "\n",
    "def injury_to_df(root_path, partecipants):\n",
    "    dfs = []\n",
    "    for p_id in partecipants: \n",
    "        p_folder = root_path + \"p{:02d}\".format(p_id) + '/pmsys/'\n",
    "        file = 'injury.csv'\n",
    "        \n",
    "        # provo ad aprire il file, se non c'è salto quel partecipante\n",
    "        try:\n",
    "            _ = open(p_folder + file)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        with open(p_folder + file) as file:\n",
    "            lines = csv.reader(file, delimiter=',')\n",
    "        \n",
    "            first_line = True\n",
    "            final_dict_list = []\n",
    "            for l in lines:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    continue\n",
    "                else:\n",
    "                    inj_dict = dict(eval(l[1]))\n",
    "                    \n",
    "                    # creo tante righe quanti sono gli infortuni (avranno lo stesso TS)\n",
    "                    for inj in inj_dict:                        \n",
    "                        final_dict = {}\n",
    "                        final_dict['TS'] = du.parser.parse(l[0])\n",
    "                        final_dict['injury'] = inj\n",
    "                        final_dict['severity'] = inj_dict[inj]\n",
    "                        final_dict_list.append(final_dict)\n",
    "\n",
    "        if len(final_dict_list) > 0:\n",
    "            df = pd.DataFrame.from_dict(final_dict_list)\n",
    "            df['partecipant'] = p_id\n",
    "            df = df.set_index(['partecipant','TS'])\n",
    "            dfs.append(df)\n",
    "    r = pd.concat(dfs)\n",
    "    r = r.sort_index()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2344ee",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "Richiamiamo le funzioni create per ciascun file del tipo:\n",
    "- calories.json\n",
    "- sedentary_minutes.json\n",
    "- distance.json\n",
    "- sleep.json\n",
    "- exercise.json\n",
    "- heart_rate.json\n",
    "- steps.json\n",
    "- lightly_active_minutes.json\n",
    "- time_in_heart_rate_zones.json\n",
    "- moderately_active_minutes.json\n",
    "- very_active_minutes.json\n",
    "- resting_heart_rate.json\n",
    "- injury.csv\n",
    "- srpe.csv\n",
    "- wellness.csv\n",
    "\n",
    "Tali funzioni sono state adattate in seguito all'ispezione dei campi che ci ha permesso di identificare le informazioni più rilevanti. Ad esempio, nei file del tipo `resting_heart_rate` era presente un campo con una data ridondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78fdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedentary_minutes = sedentary_minutes_to_df(PATH, people)\n",
    "\n",
    "sleep_0, sleep_1 = sleep_to_df(PATH, people)\n",
    "exercise_0, exercise_1 = exercise_to_df(PATH, people)\n",
    "\n",
    "lightly_active_minutes = lightly_active_minutes_to_df(PATH, people)\n",
    "time_in_heart_rate_zones = time_in_heart_rate_zones_to_df(PATH, people)\n",
    "moderately_active_minutes = moderately_active_minutes_to_df(PATH, people)\n",
    "very_active_minutes = very_active_minutes_to_df(PATH, people)\n",
    "resting_heart_rate = resting_heart_rate_to_df(PATH, people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d004f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "srpe = srpe_to_df(PATH, people)\n",
    "wellness = wellness_to_df(PATH, people)\n",
    "injury = injury_to_df(PATH, people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2a6b7",
   "metadata": {},
   "source": [
    "### Creiamo e poi salviamo i dataframe più grossi\n",
    "\n",
    "Verranno posizionati nella directory `dataframes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = steps_to_df(PATH, people)\n",
    "distance = distance_to_df(PATH, people)\n",
    "calories = calories_to_df(PATH, people)\n",
    "heart_rate = heart_rate_to_df(PATH, people)\n",
    "\n",
    "steps.to_pickle(\"dataframes/steps.pkl\")\n",
    "distance.to_pickle(\"dataframes/distance.pkl\")\n",
    "calories.to_pickle(\"dataframes/calories.pkl\")\n",
    "heart_rate.to_pickle(\"dataframes/heart_rate.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b2820",
   "metadata": {},
   "source": [
    "### Studio degli eventi Exercise e Sleep\n",
    "\n",
    "Guardando le righe riguardanti l'aver fatto o meno esercizio si osserva che quelle che hanno fatto esercizio hanno più del 90% di probabilità di avere tre attributi considerati problematici (in quanto non tutte le attività presentano un valore per `elavation_gain`, `steps` e `average_heart_rate`). Per le righe che non hanno fatto esercizio, inoltre, si osserva che la maggior parte delle volte esse non presentano il valore per questi tre attributi. Quindi, negli altri casi, si è deciso di procedere a togliere direttamente i valori perché posseduti dalla minoranza delle righe. Alla fine avremo perciò due dataset: il primo ha le righe che hanno fatto esercizio, mentre il secondo ha quelle che non hanno fatto esercizio.\n",
    "\n",
    "Per quanto riguarda il file `sleep` invece, siccome esso contiene una suddivisione del sonno in periodi (come sonno leggero, sonno profondo, fase REM, veglia), si è deciso di scomporlo in due dataframe:\n",
    "- Il primo comprende le fasi di coloro che hanno dormito (deep, wake, light, rem).\n",
    "- Il secondo comprende le fasi di coloro che si sono svegliati durante la notte (restless, awake, asleep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703fe897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# people = [i for i in range(1,17)]\n",
    "\n",
    "# DA ESEGUIRE CON COPY_sleep_to_df\n",
    "# sleep = sleep_to_df(PATH, people)\n",
    "# index = []\n",
    "# for i in range(len(sleep)):\n",
    "#     if not np.isnan(sleep.iloc[i]['level_rem_minutes']):\n",
    "#         # controllo se ci sono righe in cui ho entrambi le classi\n",
    "#         if not np.isnan(sleep.iloc[i]['level_restless_count']):\n",
    "#             index.append(i)\n",
    "# print(len(index) / len(sleep) * 100)\n",
    "\n",
    "# DA ESEGUIRE CON COPY_exercise_to_df\n",
    "# exercise = exercise_to_df(PATH, people)\n",
    "# index = []\n",
    "# for i in range(len(exercise)):\n",
    "#     if np.isnan(exercise.iloc[i]['activity_level_sedentary_minutes']):\n",
    "#         # controllo se ci sono righe in cui ho entrambi le classi\n",
    "#         if not np.isnan(exercise.iloc[i]['average_heart_rate']):\n",
    "#             index.append(i)\n",
    "# print(len(index) / len(exercise) * 100)\n",
    "\n",
    "\n",
    "#                                     Sommario dati:\n",
    "\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (elavation_gain == NaN) -----> 6.1%\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (elavation_gain != NaN) -----> 93.9%\n",
    "\n",
    "# #######################################################################################\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (elavation_gain == NaN) -----> 99.75%\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (elavation_gain != NaN) -----> 0.25%\n",
    "\n",
    "# #######################################################################################\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (steps == NaN) -----> 6.7%\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (steps != NaN) -----> 92.7%\n",
    "\n",
    "# #######################################################################################\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (steps == NaN) -----> 99.64%\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (steps != NaN) -----> 0.36%\n",
    "\n",
    "# #######################################################################################\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (average_heart_rate == NaN) -----> 0.62%\n",
    "# ( activity_level_sedentary_minutes != NaN ) AND (average_heart_rate != NaN) -----> 99.38%\n",
    "\n",
    "# #######################################################################################\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (average_heart_rate == NaN) -----> 99.919%\n",
    "# ( activity_level_sedentary_minutes == NaN ) AND (average_heart_rate != NaN) -----> 0.081%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
